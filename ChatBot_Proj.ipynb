{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Fashion Chatbot â€” Project Overview\n",
    "\n",
    "### What this project does\n",
    "\n",
    "A retrieval-augmented chatbot that answers **store FAQs** and **product queries** by semantically searching Weaviate (with `text2vec-transformers`) and then composing grounded answers with an LLM (Together), returning product IDs and concise explanations.\n",
    "\n",
    "\n",
    "## Mental map â€” end-to-end pipeline \n",
    "\n",
    "```\n",
    "User Query\n",
    "   â”‚\n",
    "   â–¼\n",
    "answer_query(query, model)                          â†  (entry point)\n",
    "   â”‚\n",
    "   â”œâ”€ check_if_faq_or_product(query)                â†  \"FAQ\" / \"Product\"\n",
    "   â”‚\n",
    "   â”œâ”€ if FAQ:\n",
    "   â”‚     query_on_faq(query)                        â† \n",
    "   â”‚        â”œâ”€ faq_collection.query.near_text(...)      (top-5)\n",
    "   â”‚        â”œâ”€ generate_faq_layout(results)            â† (format FAQ ctx)\n",
    "   â”‚        â””â”€ generate_params_dict(PROMPT, ...)        (kwargs for LLM)\n",
    "   â”‚\n",
    "   â””â”€ if Product:\n",
    "         query_on_products(query)                   â† \n",
    "            â”œâ”€ decide_task_nature(query)                â†  (\"creative\"/\"technical\")\n",
    "            â”œâ”€ get_params_for_task(label)               â†  (decoding presets)\n",
    "            â”œâ”€ get_relevant_products_from_query(query)  â†  (near_text top-20)\n",
    "            â”‚     â””â”€ products_collection.query.near_text(...)\n",
    "            â”‚     â”œâ”€ generate_metadata_from_query(query)   â†  (LLM â†’ JSON)\n",
    "            â”‚    \n",
    "            â”‚       \n",
    "            â”œâ”€ generate_items_context(results)         â†  (format product ctx)\n",
    "            â””â”€ generate_params_dict(PROMPT, role='assistant', **presets)\n",
    "               (then call generate_with_single_input(**kwargs) to get the final answer)\n",
    "```\n",
    "\n",
    "**Why this structure?**\n",
    "\n",
    "* **Classify early** â†’ route to the right knowledge (FAQ vs Product).\n",
    "* **Retrieve first** â†’ ground the LLM on real items/answers, reduce hallucinations.\n",
    "* **Decoding presets** â†’ creative vs technical tone control.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import weaviate\n",
    "from weaviate.classes.init import Timeout\n",
    "from weaviate.classes.config import Configure, Property, DataType\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "from weaviate.classes.query import Filter\n",
    "\n",
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "from utils import (\n",
    "    generate_with_single_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to local Weaviate\n",
    "\n",
    "Creates a client to the Dockerized Weaviate instance at `http://localhost:8080` (gRPC `50051`). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to local Weaviate from Docker compose\n",
    "client = weaviate.connect_to_local(\n",
    "    host=\"localhost\",\n",
    "    port=8080,\n",
    "    grpc_port=50051,\n",
    "    skip_init_checks=True,  # tolerate /ready=503\n",
    ")\n",
    "\n",
    "client.is_ready()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets:\n",
    "\n",
    "- Product: Contains the products and their information.\n",
    "- FAQ: Contains the FAQ data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Products Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading products data\n",
    "# load it as a list of JSON files first.\n",
    "products_data = joblib.load('dataset/clothes_json.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': 'Men',\n",
       " 'masterCategory': 'Apparel',\n",
       " 'subCategory': 'Topwear',\n",
       " 'articleType': 'Shirts',\n",
       " 'baseColour': 'Navy Blue',\n",
       " 'season': 'Fall',\n",
       " 'year': 2011,\n",
       " 'usage': 'Casual',\n",
       " 'productDisplayName': 'Turtle Check Men Navy Blue Shirt',\n",
       " 'price': 67.0,\n",
       " 'product_id': 15970}"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get one example\n",
    "products_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAQ Data\n",
    "Each entry is a dictionary containing the following keys: `question`, `answer`, and `type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "faq = joblib.load(\"dataset/faq.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What are your store hours?',\n",
       "  'answer': 'Our online store is open 24/7. Customer service is available from 9:00 AM to 6:00 PM, Monday through Friday.',\n",
       "  'type': 'general information'},\n",
       " {'question': 'Where is Fashion Forward Hub located?',\n",
       "  'answer': 'Fashion Forward Hub is primarily an online store. Our corporate office is located at 123 Fashion Lane, Trend City, Style State.',\n",
       "  'type': 'general information'}]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get an example\n",
    "faq[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM call Func (Together) + test\n",
    "\n",
    "\n",
    "**What it does**\n",
    "\n",
    "* Builds one `payload` in OpenAI chat format:\n",
    "  `{\"model\": ..., \"messages\": [{\"role\": role, \"content\": prompt}], ...}`\n",
    "* Calls the first available provider in this order:\n",
    "\n",
    "  1. **Together** if `together_api_key` arg is passed\n",
    "  2. **Together** if `TOGETHER_API_KEY` env var is set\n",
    "  3. **OpenAI** if `OPENAI_API_KEY` env var is set\n",
    "  4. Otherwise raises a â€œNo API key foundâ€ error\n",
    "* Returns a dict (from `.model_dump()`), so downstream parsing is the same regardless of provider.\n",
    "\n",
    "**Inputs youâ€™ll care about**\n",
    "\n",
    "* `prompt` (str): user prompt placed into `messages=[{\"role\": role, \"content\": prompt}]`.\n",
    "* `role` (str): usually `\"user\"`; you can pass `\"system\"` if you want the whole prompt as a system message.\n",
    "* `model` (str): default `\"meta-llama/Llama-3.2-3B-Instruct-Turbo\"` for Together; OpenAI fallback uses `\"gpt-4o-mini\"`.\n",
    "* `max_tokens`, `temperature`, `top_p`: standard decoding controls.\n",
    "* `**kwargs`: forwarded to the provider call (use sparingly; keep OpenAI/Together compatibility in mind).\n",
    "\n",
    "**Environment variables**\n",
    "\n",
    "* `TOGETHER_API_KEY` â†’ tries Together first.\n",
    "* `OPENAI_API_KEY` â†’ used only if Together is not available.\n",
    "\n",
    "**Return shape (OpenAI-style)**\n",
    "\n",
    "* `result[\"choices\"][0][\"message\"][\"content\"]` â†’ assistant text\n",
    "* `result[\"usage\"][\"total_tokens\"]` (if provided by the SDK)\n",
    "* `result[\"model\"]` â†’ model identifier actually used\n",
    "\n",
    "\n",
    "**Smoke test at the bottom**\n",
    "\n",
    "* Calls the function with a short prompt and pretty-prints the JSON so you can confirm the provider, `choices`, and `usage` fields look right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"oE37omS-57nCBj-98a19b24edeecb0d\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1759716455,\n",
      "  \"model\": \"meta-llama/Llama-3.2-3B-Instruct-Turbo\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"seed\": 15704831269139120000,\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"The primary colors are:\\n\\n1. Red\\n2. Blue\\n3. Yellow\\n\\nThese colors cannot be created by mixing other colors together, and they are the base colors used to create all other colors.\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"prompt\": [],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 41,\n",
      "    \"completion_tokens\": 42,\n",
      "    \"total_tokens\": 83,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# The output is a dictionary containing the role and content from the LLM call, as well as the token usage.:\n",
    "result = generate_with_single_input(\"What are the primary colors?\")\n",
    "print(json.dumps(result, indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to retreive just the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The primary colors are:\n",
      "\n",
      "1. Red\n",
      "2. Blue\n",
      "3. Yellow\n",
      "\n",
      "These colors cannot be created by mixing other colors together, and they are the base colors used to create all other colors.\n"
     ]
    }
   ],
   "source": [
    "# Retreive just the content\n",
    "print(result['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To retrieve the total number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "# The total tokens count (input + output) for this is:\n",
    "print(result['usage']['total_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to generate the parameters dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_params_dict(\n",
    "    prompt: str,\n",
    "    temperature: float = 1.0,\n",
    "    role: str = 'user',\n",
    "    top_p: float = 1.0,\n",
    "    max_tokens: int = 500,\n",
    "    model: str = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generates a dictionary of parameters for calling a Language Learning Model (LLM),\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing all specified parameters which can then be used to configure and execute a call to the LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    kwargs = {\n",
    "        \"prompt\": prompt,\n",
    "        \"role\": role,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"model\": model\n",
    "    }\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Solve 3x^2 + 5 = 0', 'role': 'user', 'temperature': 1.0, 'top_p': 1.0, 'max_tokens': 500, 'model': 'meta-llama/Llama-3.2-3B-Instruct-Turbo'}\n"
     ]
    }
   ],
   "source": [
    "kwargs = generate_params_dict(\"Solve 3x^2 + 5 = 0\")\n",
    "print(kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: To solve the equation 3x^2 + 5 = 0, we need to isolate x.\n",
      "\n",
      "First, subtract 5 from both sides:\n",
      "\n",
      "3x^2 = -5\n",
      "\n",
      "Next, divide both sides by 3:\n",
      "\n",
      "x^2 = -5/3\n",
      "\n",
      "Since the square of any real number cannot be negative, there are no real solutions to the equation. However, we can express the complex solutions as:\n",
      "\n",
      "x = Â±iâˆš(5/3)\n",
      "\n",
      "where i is the imaginary unit (i = âˆš(-1)).\n",
      "\n",
      "Total Tokens: 159\n"
     ]
    }
   ],
   "source": [
    "# call the LLM \n",
    "result = generate_with_single_input(**kwargs)\n",
    "content = result['choices'][0]['message']['content']\n",
    "total_tokens = result['usage']['total_tokens']\n",
    "print(f\"Content: {content}\\n\\nTotal Tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify query as **FAQ** vs **Product**\n",
    "\n",
    "Uses a short LLM prompt (low temperature, small max_tokens) to label an input `query` as either **FAQ** (store policies, contact, hours, etc.) or **Product** (item-specific info, recommendations, filters). \n",
    "Returns the normalized label (`\"FAQ\"` / `\"Product\"` or `\"undefined\"`) plus the modelâ€™s `total_tokens`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def check_if_faq_or_product(query):\n",
    "    \"\"\"\n",
    "    Determines whether a given instruction prompt is related to a frequently asked question (FAQ) or a product inquiry.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The instruction or query that needs to be labeled as either FAQ or Product related.\n",
    "    - simplified (bool): If True, uses a simplified prompt.\n",
    "\n",
    "    Returns:\n",
    "    - str: The label 'FAQ' if the prompt is deemed a frequently asked question, 'Product' if it is related to product information, or\n",
    "      None if the label is inconclusive.\n",
    "    \"\"\"\n",
    " \n",
    "\n",
    "    PROMPT = f\"\"\"Label the following instruction as an FAQ related answer or a product related answer for a clothing store.\n",
    "    Product related answers are answers specific about product information or that needs to use the products to give an answer.\n",
    "    Examples:\n",
    "            Is there a refund for incorrectly bought clothes? Label: FAQ\n",
    "            Where are your stores located?: Label: FAQ\n",
    "            Tell me about the cheapest T-shirts that you have. Label: Product\n",
    "            Do you have blue T-shirts under 100 dollars? Label: Product\n",
    "            What are the available sizes for the t-shirts? Label: FAQ\n",
    "            How can I contact you via phone? Label: FAQ\n",
    "            How can I find the promotions? Label: FAQ\n",
    "            Give me ideas for a sunny look. Label: Product\n",
    "    Return only one of the two labels: FAQ or Product, nothing more.\n",
    "    Query to classify: {query}\n",
    "                \"\"\"\n",
    "        \n",
    "    # Get the kwargs dictinary to call the llm, with PROMPT as prompt, low temperature (0 or near 0) and max_tokens = 10\n",
    "    kwargs = generate_params_dict(PROMPT, temperature = 0, max_tokens = 10)\n",
    "\n",
    "    response = generate_with_single_input(**kwargs) \n",
    "    label = response['choices'][0]['message']['content']\n",
    "    total_tokens = response['usage']['total_tokens']\n",
    "\n",
    "    if 'faq' in label.lower():\n",
    "        label = 'FAQ'\n",
    "    elif 'product' in label.lower():\n",
    "        label = 'Product'\n",
    "    else:\n",
    "        label = 'undefined'\n",
    "\n",
    "    return label, total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format a list of FAQs into a prompt-ready block\n",
    "\n",
    "Takes a list of FAQ dicts (`{'question','answer','type'}`) and builds a single newline-separated string like `Question: â€¦ Answer: â€¦ Type: â€¦` per item. Useful for stuffing the most relevant FAQs into an LLM prompt in a compact, readable form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_faq_layout(faq_dict):\n",
    "    \"\"\"\n",
    "    Generates a formatted string layout for a list of FAQs.\n",
    "\n",
    "    This function iterates through a dictionary of frequently asked questions (FAQs) and constructs\n",
    "    a string where each question is followed by its corresponding answer and type.\n",
    "\n",
    "    Parameters:\n",
    "    - faq_dict (list): A list of dictionaries, each containing keys 'question', 'answer', and 'type' \n",
    "      representing an FAQ entry.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string representing the formatted layout of FAQs, with each entry on a separate line.\n",
    "    \"\"\"\n",
    "    # Initialize an empty string\n",
    "    t = \"\"\n",
    "    \n",
    "    # Iterate over every FAQ question in the FAQ list\n",
    "    for f in faq_dict:\n",
    "        # Append the question with formatted string (remember to use f-string and access the values as f['question'], f['answer'] and so on)\n",
    "        # Also, do not forget to add a new line character (\\n) at the end of each line.\n",
    "        t += f\"Question: {f['question']} Answer: {f['answer']} Type: {f['type']}\\n\" \n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are your store hours? Answer: Our online store is open 24/7. Customer service is available from 9:00 AM to 6:00 PM, Monday through Friday. Type: general information\n",
      "Question: Where is Fashion Forward Hub located? Answer: Fashion Forward Hub is primarily an online store. Our corporate office is located at 123 Fashion Lane, Trend City, Style State. Type: general information\n",
      "Question: Do you have a physical store location? Answer: At this time, we operate exclusively online. This allows us to offer a broader selection and lower prices directly to you. Type: general information\n",
      "Question: How can I create an account with Fashion Forward Hub? Answer: Click on 'Sign Up' in the top right corner of our website and follow the instructions to set up your account. Type: general information\n",
      "Question: How do I subscribe to your newsletter? Answer: To receive the latest updates and promotions, sign up for our newsletter at the bottom of our homepage. Type: general information\n",
      "Question:\n"
     ]
    }
   ],
   "source": [
    "# You can generate a full faq_layout with the entire FAQ questions\n",
    "faq_layout = generate_faq_layout(faq)\n",
    "print(faq_layout[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vector DB: Create **Faq** collection (vectorized) and batch-insert FAQs\n",
    "\n",
    "Drops any existing `Faq` collection, recreates it with the `text2vec-transformers` vectorizer (enabling `near_text` search), defines `question/answer/type` properties, then batch-inserts all FAQ items using stable UUIDs derived from each question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weaviate.collections.collection.sync.Collection at 0x188d2467050>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from weaviate.util import generate_uuid5\n",
    "\n",
    "try:\n",
    "    if client.collections.exists(\"Faq\"):\n",
    "        client.collections.delete(\"Faq\")\n",
    "except Exception as e:\n",
    "    print(\"Warning deleting Faq:\", e)\n",
    "\n",
    "client.collections.create(\n",
    "    name=\"Faq\",\n",
    "    vectorizer_config=Configure.Vectorizer.text2vec_transformers(),  \n",
    "    properties=[\n",
    "        Property(name=\"question\", data_type=DataType.TEXT),\n",
    "        Property(name=\"answer\",   data_type=DataType.TEXT),\n",
    "        Property(name=\"type\",     data_type=DataType.TEXT),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert FAQ docs\n",
    "faq_collection = client.collections.get(\"Faq\")\n",
    "with faq_collection.batch.fixed_size(batch_size=20, concurrent_requests=5) as batch:\n",
    "    for document in faq:  # [{'question':..., 'answer':..., 'type':...}, ...]\n",
    "        uuid = generate_uuid5(document['question'])\n",
    "        batch.add_object(properties=document, uuid=uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic search over FAQs\n",
    "\n",
    "Runs a `near_text` query against the vectorized `Faq` collection to fetch the top 5 relevant Q&As for â€œWhat is the return policy?â€, then prints `question -> answer`. (Assumes `Faq` is created with `text2vec-transformers` and populated.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is your return policy timeframe? -> We accept returns within 30 days of delivery. Conditions apply for specific categories like accessories.\n",
      "How long does it take to process a return? -> Return processing typically takes 5-7 business days from when the item is received at our warehouse.\n",
      "Are return shipping costs covered? -> We provide a prepaid return label for domestic returns. For international returns, shipping is at the customer's cost.\n",
      "Can I return a sale item? -> Sale items are final sale and cannot be returned or exchanged, unless stated otherwise.\n",
      "How do I exchange an item? -> Initiate an exchange through our Returns Center, selecting the item you wish to exchange and the desired replacement.\n"
     ]
    }
   ],
   "source": [
    "# Semantic search\n",
    "res = faq_collection.query.near_text(query=\"What is the return policy?\", limit=5)\n",
    "for obj in res.objects:\n",
    "    print(obj.properties[\"question\"], \"->\", obj.properties[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build prompt from top-5 semantic FAQ hits\n",
    "\n",
    "Looks up the 5 most relevant FAQ entries via `near_text`, reverses them (least â†’ most relevant at the bottom) to bias recency in the prompt, renders a compact FAQ layout, and returns **LLM call parameters** (`kwargs`) using `generate_params_dict`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def query_on_faq(query, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a prompt to query an FAQ system and generates a response.\n",
    "\n",
    "    This function integrates an FAQ layout into the prompt to help generate a suitable answer to the given query\n",
    "    using a language model. It uses semantic search to extract a relevant subset of FAQ questions\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The query about which the function seeks to provide an answer from the FAQ.\n",
    "    - **kwargs: Optional keyword arguments for extra configuration of prompt parameters.\n",
    "\n",
    "    Returns:\n",
    "    - str: The response generated from the language model based on the input query and FAQ layout.\n",
    "\n",
    "    \"\"\"    \n",
    "\n",
    "    # Get the 5 most relevant FAQ objects, in this case limit = None\n",
    "    results = faq_collection.query.near_text(f\"{query}\", limit = 5)    \n",
    "\n",
    "    # Transform the results in a list of dictionary\n",
    "    results = [x.properties for x in results.objects] \n",
    "    # Reverse the order to add the most relevant objects in the bottom, so it gets closer to the end of the input\n",
    "    results.reverse() \n",
    "    # Generate the faq layout with the new list of FAQ questions `results`\n",
    "    faq_layout = generate_faq_layout(results) \n",
    "\n",
    "    # Different prompt to deal with this new scenario. \n",
    "    PROMPT = (f\"You will be provided with a query for a clothing store regarding FAQ. It will be provided relevant FAQ from the clothing store.\" \n",
    "    f\"Answer the query based on the relevant FAQ provided. They are ordered in decreasing relevance, so the first is the most relevant FAQ and the last is the least relevant.\"  \n",
    "    f\"Answer the instruction based on them. You might use more than one question and answer to make your answer. Only answer the question and do not mention that you have access to a FAQ.\\n\"  \n",
    "    f\"<FAQ>\\n\"  \n",
    "    f\"RELEVANT FAQ ITEMS:\\n{faq_layout}\\n\"  \n",
    "    f\"</FAQ>\\n\" \n",
    "    f\"Query: {query}\")   \n",
    " \n",
    "    \n",
    "    # Generate the parameters dict with PROMPT and **kwargs \n",
    "    kwargs = generate_params_dict(PROMPT, **kwargs) \n",
    "\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the dictionary of arguments\n",
    "kwargs = query_on_faq(\"I received the dress I ordered but I don't like it. How can I return it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n"
     ]
    }
   ],
   "source": [
    "# The number of split tokens in this prompt is:\n",
    "print(len(kwargs['prompt'].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the inference\n",
    "result = generate_with_single_input(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To return your dress, you can initiate the return process through our Returns Center. Select the item you wish to return, and then select the desired replacement you would like to exchange it for. Once you initiate the return, please allow 5-7 business days for return processing. \n",
      "\n",
      "Additionally, you should note that it is generally not possible to return a sale item, as they are specified as final sale and cannot be returned or exchanged unless the specific policy allows for it. However, in your case, since the item is not a sale item, you should be able to return it for a refund or exchange.\n"
     ]
    }
   ],
   "source": [
    "print(result['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n"
     ]
    }
   ],
   "source": [
    "# Get the total tokens\n",
    "print(result['usage']['total_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify query as **creative** vs **technical**\n",
    "\n",
    "Prompts the LLM (low temperature, `max_tokens=1`) to label a user query as **creative** (suggestions, styling ideas) or **technical** (product details, availability, prices). Returns the raw label text from the model and `total_tokens` for telemetry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def decide_task_nature(query):\n",
    "    \"\"\"\n",
    "    Determines the nature of a query, labeling it as either creative or technical.\n",
    "    This function constructs a prompt for a language model to decide if a given query requires a creative response,\n",
    "    such as making suggestions or composing ideas, or a technical response, like providing product details or prices.\n",
    "\n",
    "    Returns:\n",
    "    - str: The label 'creative' if the query requires creative input, or 'technical' if it requires technical information.\n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "    PROMPT = f\"\"\"Decide if the following query is a query that requires creativity (creating, composing, making new things) or technical (information about products, prices etc.). Label it as creative or technical.\n",
    "        Examples:\n",
    "        Give me suggestions on a nice look for a nightclub. Label: creative\n",
    "        What are the blue dresses you have available? Label: technical\n",
    "        Give me three Tshirts for summer. Label: technical\n",
    "        Give me a look for attending a wedding party. Label: creative\n",
    "        Give me suggestions on clothes that match a green Tshirt. Label: creative\n",
    "        I would like a suggestion on which products match a green Tshirt I already have. Label: creative\n",
    "\n",
    "        Query to be analyzed: {query}. Only output one token with the label\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    kwargs = generate_params_dict(PROMPT, temperature = 0, max_tokens = 1)\n",
    "\n",
    "    response = generate_with_single_input(**kwargs) \n",
    "\n",
    "    # Get the Label by accessing the content key of the response dictionary\n",
    "    label = response['choices'][0]['message']['content']\n",
    "    total_tokens = response['usage']['total_tokens']\n",
    "\n",
    "    return label, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries = [\"Give me two sneakers with vibrant colors.\",\n",
    "           \"What are the most expensive clothes you have in your catalogue?\",\n",
    "           \"I have a green Dress and I like a suggestion on an accessory to match with it.\",\n",
    "           \"Give me three trousers with vibrant colors you have in your catalogue.\",\n",
    "           \"Create a look for a woman walking in a park on a sunny day. It must be fresh due to hot weather.\"\n",
    "           ]\n",
    "\n",
    "labels = ['technical', 'technical', 'creative', 'technical', 'creative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Give me two sneakers with vibrant colors. Label Predicted: \u001b[32mtechnical\u001b[0m. Correct Label: technical Total Tokens: \u001b[31m196\u001b[0m\n",
      "Query: What are the most expensive clothes you have in your catalogue? Label Predicted: \u001b[32mtechnical\u001b[0m. Correct Label: technical Total Tokens: \u001b[31m200\u001b[0m\n",
      "Query: I have a green Dress and I like a suggestion on an accessory to match with it. Label Predicted: \u001b[32mcreative\u001b[0m. Correct Label: creative Total Tokens: \u001b[31m206\u001b[0m\n",
      "Query: Give me three trousers with vibrant colors you have in your catalogue. Label Predicted: \u001b[32mtechnical\u001b[0m. Correct Label: technical Total Tokens: \u001b[31m201\u001b[0m\n",
      "Query: Create a look for a woman walking in a park on a sunny day. It must be fresh due to hot weather. Label Predicted: \u001b[32mcreative\u001b[0m. Correct Label: creative Total Tokens: \u001b[31m212\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for query, correct_label in zip(queries, labels):\n",
    "    response, total_tokens = decide_task_nature(query)\n",
    "    label = response\n",
    "    if label == correct_label:\n",
    "        label = \"\\033[32m\" + label + \"\\033[0m\" \n",
    "    else:\n",
    "        label = \"\\033[31m\" + label + \"\\033[0m\"\n",
    "    if total_tokens > 170:\n",
    "        total_tokens = \"\\033[31m\"  + str(total_tokens) + \"\\033[0m\"\n",
    "    else:\n",
    "        total_tokens = \"\\033[32m\"  + str(total_tokens) + \"\\033[0m\"\n",
    "    print(f\"Query: {query} Label Predicted: {label}. Correct Label: {correct_label} Total Tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“¦ Start: **Products** section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasify Query on creative vs technical tasks\n",
    "\n",
    "Returns a small dict of `top_p` and `temperature` tuned for the task: higher randomness for **creative**, lower for **technical**, with a sensible default fallback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def get_params_for_task(task):\n",
    "    \"\"\"\n",
    "    Retrieves specific language model parameters based on the task nature.\n",
    "\n",
    "    This function provides parameter sets tailored for creative or technical tasks to optimize\n",
    "    language model behavior. For creative tasks, higher randomness is encouraged, while technical\n",
    "    tasks are handled with more focus and precision. A default parameter set is provided for unexpected cases.\n",
    "\n",
    "    Parameters:\n",
    "    - task (str): The nature of the task ('creative' or 'technical').\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing 'top_p' and 'temperature' settings for the specified task.\n",
    "    \"\"\"\n",
    "    # Create the parameters dict for technical and creative tasks\n",
    "    PARAMETERS_DICT = {\"creative\": {'top_p': 0.9, 'temperature': 1},\n",
    "                       \"technical\": {'top_p': 0.7, 'temperature': 0.3}} \n",
    "    \n",
    "    # If task is technical, return the value for the key technical in PARAMETERS_DICT\n",
    "    if task == 'technical':\n",
    "        param_dict = PARAMETERS_DICT['technical'] \n",
    "\n",
    "    # If task is creative, return the value for the key creative in PARAMETERS_DICT\n",
    "    elif task == 'creative':\n",
    "        param_dict = PARAMETERS_DICT['creative'] \n",
    "\n",
    "    # If task is a different value, fallback to another set of parameters\n",
    "    else: \n",
    "        param_dict = {'top_p': 0.5, 'temperature': 1} \n",
    "\n",
    "    \n",
    "    return param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': 'Men',\n",
       " 'masterCategory': 'Apparel',\n",
       " 'subCategory': 'Topwear',\n",
       " 'articleType': 'Shirts',\n",
       " 'baseColour': 'Navy Blue',\n",
       " 'season': 'Fall',\n",
       " 'year': 2011,\n",
       " 'usage': 'Casual',\n",
       " 'productDisplayName': 'Turtle Check Men Navy Blue Shirt',\n",
       " 'price': 67.0,\n",
       " 'product_id': 15970}"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's remember the data structure of a product\n",
    "products_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a dictionary with every possible value for the categories the LLM can pick from to generate a JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to generate the dictionary with the possible values for each key\n",
    "values = {}\n",
    "for d in products_data:\n",
    "    for key, val in d.items():\n",
    "        if key in ('product_id', 'price', 'productDisplayName', 'subCategory', 'year'):\n",
    "            continue\n",
    "        if key not in values.keys():\n",
    "            values[key] = set()\n",
    "        values[key].add(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'All seasons', 'Fall', 'Spring', 'Summer', 'Winter'}"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of possible values for the feature 'season'\n",
    "values['season']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate metadata JSON for product filtering\n",
    "\n",
    "Prompts the LLM (low temperature) to produce a **strict JSON** describing filters (gender, masterCategory, articleType, baseColour, price, usage, season) using only allowed values from `values`. Returns the JSON **string** plus `total_tokens`; you can then parse/validate it and apply as filters in the `Products` collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_metadata_from_query(query):\n",
    "    \"\"\"\n",
    "    Generates metadata in JSON format based on a given query to filter clothing items.\n",
    "\n",
    "    This function constructs a prompt for a language model to create a JSON object that will\n",
    "    guide the filtering of a vector database query for clothing items. It takes possible values from\n",
    "    a predefined set and ensures only relevant metadata is included in the output JSON.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The query describing specific clothing-related needs.\n",
    "\n",
    "    Returns:\n",
    "    - str: A JSON string representing metadata with keys like gender, masterCategory, articleType,\n",
    "      baseColour, price, usage, and season. Each value in the JSON is within a list, with prices specified\n",
    "      as a dict containing \"min\" and \"max\" values. Unrestricted keys should use [\"Any\"] and unspecified\n",
    "      prices should default to {\"min\": 0, \"max\": \"inf\"}.\n",
    "    \"\"\"\n",
    "\n",
    "    PROMPT = f\"\"\"\n",
    "    One query will be provided. For the given query, there will be a call on vector database to query relevant clothing items. \n",
    "    Generate a JSON with useful metadata to filter the products in the query. Possible values for each feature is in the following json: \n",
    "    {values}\n",
    "\n",
    "    Provide a JSON with the features that best fit in the query (can be more than one, write in a list). Also, if present, add a price key, saying if there is a price range (between values, greater than or smaller than some value).\n",
    "    Only return the JSON, nothing more. price key must be a JSON with \"min\" and \"max\" values (0 if no lower bound and inf if no upper bound). \n",
    "    Always include gender, masterCategory, articleType, baseColour, price, usage and season as keys. All values must be within lists.\n",
    "    If there is no price set, add min = 0 and max = inf.\n",
    "    Only include values that are given in the json above. \n",
    "    \n",
    "    Example of expected JSON:\n",
    "\n",
    "    {{\n",
    "    \"gender\": [\"Women\"],\n",
    "    \"masterCategory\": [\"Apparel\"],\n",
    "    \"articleType\": [\"Dresses\"],\n",
    "    \"baseColour\": [\"Blue\"],\n",
    "    \"price\": {{\"min\": 0, \"max\": \"inf\"}},\n",
    "    \"usage\": [\"Formal\"],\n",
    "    \"season\": [\"All seasons\"]\n",
    "    }}\n",
    "\n",
    "    Query: {query}\n",
    "             \"\"\"\n",
    "    \n",
    "    # Generate the response with the generate_with_single_input, PROMPT, temperature = 0 (low randomness) and max_tokens = 1500.\n",
    "    kwargs = {\"prompt\": PROMPT, 'temperature': 0, \"max_tokens\": 1500} \n",
    "\n",
    "    response = generate_with_single_input(**kwargs) \n",
    "\n",
    "    # Get the Label by accessing the content key of the response dictionary\n",
    "    content = response['choices'][0]['message']['content']\n",
    "    total_tokens = response['usage']['total_tokens']\n",
    " \n",
    "\n",
    "    \n",
    "    return content, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gender\": [\"Men\"],\n",
      "    \"masterCategory\": [\"Apparel\"],\n",
      "    \"articleType\": [\"Shirts\", \"Shorts\"],\n",
      "    \"baseColour\": [\"Yellow\", \"Orange\", \"Green\", \"Blue\", \"White\"],\n",
      "    \"price\": {\"min\": 0, \"max\": 300},\n",
      "    \"usage\": [\"Casual\", \"Smart Casual\"],\n",
      "    \"season\": [\"Summer\"]\n",
      "}\n",
      "1460\n"
     ]
    }
   ],
   "source": [
    "content, total_tokens = generate_metadata_from_query(\"Create a look for a man that suits a sunny day in the park. I don't want to spend more than 300 dollars on each piece.\")\n",
    "print(content)\n",
    "print(total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vectorized **Products** collection\n",
    "\n",
    "Drops any existing `Products` class and recreates it with the `text2vec-transformers` vectorizer so `near_text` works out of the box. Defines core product attributes (text fields + numeric `year`, `price`, and `product_id`) and returns a handle to the collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Drop & recreate to ensure it has a vectorizer\\ntry:\\n    if client.collections.exists(\"Products\"):\\n        client.collections.delete(\"Products\")\\nexcept Exception as e:\\n    print(\"Warning while deleting Products:\", e)\\n\\nclient.collections.create(\\n    name=\"Products\",\\n    vectorizer_config=Configure.Vectorizer.text2vec_transformers(),  # <-- key for near_text\\n    properties=[\\n        Property(name=\"productDisplayName\", data_type=DataType.TEXT),\\n        Property(name=\"articleType\",        data_type=DataType.TEXT),\\n        Property(name=\"baseColour\",         data_type=DataType.TEXT),\\n        Property(name=\"gender\",             data_type=DataType.TEXT),\\n        Property(name=\"season\",             data_type=DataType.TEXT),\\n        Property(name=\"usage\",              data_type=DataType.TEXT),\\n        Property(name=\"masterCategory\",     data_type=DataType.TEXT),\\n        Property(name=\"subCategory\",        data_type=DataType.TEXT),\\n        Property(name=\"year\",               data_type=DataType.INT),\\n        Property(name=\"price\",              data_type=DataType.NUMBER),\\n        Property(name=\"product_id\",         data_type=DataType.INT),\\n    ],\\n)\\n\\nproducts_collection = client.collections.get(\"Products\")\\n'"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Drop & recreate to ensure it has a vectorizer\n",
    "try:\n",
    "    if client.collections.exists(\"Products\"):\n",
    "        client.collections.delete(\"Products\")\n",
    "except Exception as e:\n",
    "    print(\"Warning while deleting Products:\", e)\n",
    "\n",
    "client.collections.create(\n",
    "    name=\"Products\",\n",
    "    vectorizer_config=Configure.Vectorizer.text2vec_transformers(),  # <-- key for near_text\n",
    "    properties=[\n",
    "        Property(name=\"productDisplayName\", data_type=DataType.TEXT),\n",
    "        Property(name=\"articleType\",        data_type=DataType.TEXT),\n",
    "        Property(name=\"baseColour\",         data_type=DataType.TEXT),\n",
    "        Property(name=\"gender\",             data_type=DataType.TEXT),\n",
    "        Property(name=\"season\",             data_type=DataType.TEXT),\n",
    "        Property(name=\"usage\",              data_type=DataType.TEXT),\n",
    "        Property(name=\"masterCategory\",     data_type=DataType.TEXT),\n",
    "        Property(name=\"subCategory\",        data_type=DataType.TEXT),\n",
    "        Property(name=\"year\",               data_type=DataType.INT),\n",
    "        Property(name=\"price\",              data_type=DataType.NUMBER),\n",
    "        Property(name=\"product_id\",         data_type=DataType.INT),\n",
    "    ],\n",
    ")\n",
    "\n",
    "products_collection = client.collections.get(\"Products\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch-ingest cleaned `products_data` with stable UUIDs\n",
    "\n",
    "Cleans each product (safe casting for `year/price/product_id`), then batch-inserts into the vectorized `Products` collection with a fixed batch size and concurrency. UUIDs are derived from `product_id` (fallback to name) for idempotent re-runs. Weaviateâ€™s `text2vec-transformers` auto-vectorizes on insert, enabling `near_text` queries later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44424/44424 [00:03<00:00, 12989.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "def _to_int(x):\n",
    "    try:\n",
    "        if x is None or (isinstance(x, float) and math.isnan(x)) or (isinstance(x, str) and not x.strip()):\n",
    "            return None\n",
    "        return int(float(x))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _to_float(x):\n",
    "    try:\n",
    "        if x is None or (isinstance(x, float) and math.isnan(x)) or (isinstance(x, str) and not x.strip()):\n",
    "            return None\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _clean_obj(d):\n",
    "    return {\n",
    "        \"productDisplayName\": d.get(\"productDisplayName\"),\n",
    "        \"articleType\":        d.get(\"articleType\"),\n",
    "        \"baseColour\":         d.get(\"baseColour\"),\n",
    "        \"gender\":             d.get(\"gender\"),\n",
    "        \"season\":             d.get(\"season\"),\n",
    "        \"usage\":              d.get(\"usage\"),\n",
    "        \"masterCategory\":     d.get(\"masterCategory\"),\n",
    "        \"subCategory\":        d.get(\"subCategory\"),\n",
    "        \"year\":               _to_int(d.get(\"year\")),\n",
    "        \"price\":              _to_float(d.get(\"price\")),\n",
    "        \"product_id\":         _to_int(d.get(\"product_id\")),\n",
    "    }\n",
    "\n",
    "\n",
    "with products_collection.batch.fixed_size(batch_size=200, concurrent_requests=4) as batch:\n",
    "    for d in tqdm(products_data):\n",
    "        obj = _clean_obj(d)\n",
    "        # use a stable UUID; product_id preferred, else fallback to name\n",
    "        key = str(obj.get(\"product_id\") or obj.get(\"productDisplayName\") or repr(obj))\n",
    "        batch.add_object(properties=obj, uuid=generate_uuid5(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44424"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(products_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5-3'></a>\n",
    "\n",
    "<a id='5-3'></a>\n",
    "### 5.3 Filtering by Metadata\n",
    "\n",
    "The functions used to filter by metadata have been moved to the **`utils.py`** file.\n",
    "You can find this file in the **File Browser** on the left panel.\n",
    "\n",
    "You worked with these functions in the previous assignment, but for this one, **you wonâ€™t need to use them directly**.\n",
    "\n",
    "So, letâ€™s go ahead and jump into the exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic product search (top-20)\n",
    "\n",
    "Queries the vectorized `Products` collection with `near_text` using the userâ€™s `query`, returning the top 20 matched objects. No LLM is called here, so token usage is `0`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def get_relevant_products_from_query(query):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant products for a given query by applying semantic search.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The query string used to search for relevant products.\n",
    "    Returns:\n",
    "    list: A list of product objects that are most relevant to the query.\n",
    "    total_tokens: The number of tokens used in the LLM call. Returns 0 if simplified search is used.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    #do a semantic search with 20 objects and return it           \n",
    "    results = products_collection.query.near_text(f\"{query}\", limit=20)  \n",
    "        \n",
    "    return results.objects, 0  # Return the objects and 0 tokens since no LLM call was made\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Give me three T-shirts to use in sunny days\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "t, total_tokens = get_relevant_products_from_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile product results into a prompt-friendly context block\n",
    "\n",
    "Takes a list of Weaviate result objects and builds a multi-line string with key attributes (ID, name, category, usage, gender, type, color, season, year) per productâ€”handy to feed into an LLM. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_items_context(results):\n",
    "    \"\"\"\n",
    "    Compile detailed product information from a list of result objects into a formatted string.\n",
    "\n",
    "    Parameters:\n",
    "    results (list): A list of result objects, each having a `properties` attribute that is a dictionary \n",
    "                    containing product attributes such as 'product_id', 'productDisplayName', \n",
    "                    'masterCategory', 'usage', 'gender', 'articleType', 'subCategory', \n",
    "                    'baseColour', 'season', and 'year'.\n",
    "\n",
    "    Returns:\n",
    "    str: A multi-line string where each line contains the formatted details of a single product.\n",
    "         Each product detail includes the product ID, name, category, usage, gender, type, color, \n",
    "         season, and year.\n",
    "    \"\"\"\n",
    "    t = \"\"  # Initialize an empty string to accumulate product information\n",
    "\n",
    "    for item in results:  # Iterate through each item in the results list\n",
    "        item = item.properties  # Access the properties dictionary of the current item\n",
    "\n",
    "        # Append formatted product details to the output string\n",
    "        t += (\n",
    "            f\"Product ID: {item['product_id']}. \"\n",
    "            f\"Product name: {item['productDisplayName']}. \"\n",
    "            f\"Product Category: {item['masterCategory']}. \"\n",
    "            f\"Product usage: {item['usage']}. \"\n",
    "            f\"Product gender: {item['gender']}. \"\n",
    "            f\"Product Type: {item['articleType']}. \"\n",
    "            f\"Product Category: {item['subCategory']} \"\n",
    "            f\"Product Color: {item['baseColour']}. \"\n",
    "            f\"Product Season: {item['season']}. \"\n",
    "            f\"Product Year: {item['year']}.\\n\"\n",
    "        )\n",
    "\n",
    "    return t  # Return the complete formatted string with product details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product ID: 35885. Product name: Jungle Book Boys Yee-Ha! White T-shirt. Product Category: Apparel. Product usage: Casual. Product gender: Boys. Product Type: Tshirts. Product Category: Topwear Product Color: White. Product Season: Summer. Product Year: 2012.\n",
      "Product ID: 36325. Product name: Mr.Men Boys Mr. Happy White T-shirt. Product Category: Apparel. Product usage: Casual. Product gender: Boys. Product Type: Tshirts. Product Category: Topwear Product Color: White. Product Season: Summer. Product Year: 2012.\n",
      "Product ID: 36324. Product name: Mr.Men Boys Mr. Happy White T-shirt. Product Category: Apparel. Product usage: Casual. Product gender: Boys. Product Type: Tshirts. Product Category: Topwear Product Color: White. Product Season: Summer. Product Year: 2012.\n",
      "Product ID: 12226. Product name: Basics Men Pack Of 3 T-shirts. Product Category: Apparel. Product usage: Casual. Product gender: Men. Product Type: Tshirts. Product Category: Topwear Product Color: White. Product Season: Summ\n"
     ]
    }
   ],
   "source": [
    "print(generate_items_context(t)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LLM prompt for product answers (creative/technical routing)\n",
    "\n",
    "Classifies the query (creative vs technical), sets decoding params, retrieves top-20 relevant products via semantic search, composes a compact product-context block, and returns `kwargs` (prompt + params) for the LLM along with total token count used for routing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_on_products(query):\n",
    "    \"\"\"\n",
    "    Execute a product query process to generate a response based on the nature of the query.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The input query string that needs to be analyzed and answered using product data.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary of keyword arguments (`kwargs`) containing the prompt and additional settings \n",
    "          for creating a response, suitable for input to an LLM or other processing system.\n",
    "    int: Number of tokens used in the process to create the kwargs dictionary\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    # Determine if the query is technical or creative in nature\n",
    "    \n",
    "    query_label, tokens = decide_task_nature(query)\n",
    "    \n",
    "    # Sum the tokens used to decide the task nature (creative or technical)\n",
    "    total_tokens += tokens\n",
    "\n",
    "    # Obtain necessary parameters based on the query type\n",
    "    parameters_dict = get_params_for_task(query_label)\n",
    "    \n",
    "    # Retrieve products that are relevant to the query\n",
    "    relevant_products, tokens = get_relevant_products_from_query(query)\n",
    "    \n",
    "    # Sum the tokens used to get relevant products \n",
    "    total_tokens += tokens\n",
    "     \n",
    "    # Create a context string from the relevant products\n",
    "    context = generate_items_context(relevant_products)\n",
    "\n",
    "    # Construct a prompt including product details and the query. Remember to add the context and the query in the prompt, also, ask the LLM to provide the product ID in the answer\n",
    "    PROMPT = (\n",
    "        f\"Given the available set of clothing products given by: \"\n",
    "        f\"CLOTHING PRODUCTS AVAILABLE:\\n{context}\\n\"\n",
    "        f\"Answer the question that follows.\\n\"\n",
    "        f\"Never use more than 5 clothing products available below to compose your answer.\\n\"\n",
    "        f\"Provide the item ID in your answers.\\n\"\n",
    "        f\"The other information might be provided but not necessarily all of them, pick only the relevant ones for the given query.\\n\"\n",
    "        \n",
    "        f\"QUERY: {query}\"\n",
    "    )\n",
    "    \n",
    "    # Generate kwargs (parameters dict) for parameterized input to the LLM with , Prompt, role = 'assistant' and **parameters_dict\n",
    "    kwargs = generate_params_dict(PROMPT, role='assistant', **parameters_dict) \n",
    "\n",
    "    \n",
    "    return kwargs, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "kwargs, total_tokens = query_on_products('Make a wonderful look for a man attending a wedding party happening during night.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a wonderful look for a man attending a wedding party during the night, I recommend combining the following clothing products:\n",
      "\n",
      "1. Product ID: 28365. IDEE Men Brown Sunglasses - Although it's summer, wearing sunglasses can add a touch of sophistication and elegance to a man's outfit. The brown color will complement a variety of colors and won't distract from the overall look.\n",
      "2. Product ID: 25066. Lino Perros Men Formal Turquoise Blue Accessory Gift Set - This gift set includes a tie, pocket square, and cufflinks, which can add a pop of color and personality to the outfit. The turquoise blue color will add a unique touch to a classic wedding party attire.\n",
      "3. Product ID: 49696. Park Avenue Red Checked Tie - A red tie can be a bold and eye-catching choice for a wedding party. The checked pattern will add texture and visual interest to the outfit.\n",
      "4. Product ID: 31186. Cabarelli Men Accessory Gift Set (with black color) - A simple black gift set can provide a versatile and classic element to the outfit. It can be used to add a touch of elegance or to create a monochromatic look.\n",
      "\n",
      "This combination of products will create a stylish and memorable look for the man attending the wedding party. The brown sunglasses will add a touch of sophistication, while the turquoise blue accessory gift set and red tie will add a pop of color. The black gift set will provide a versatile and classic element to the outfit.\n"
     ]
    }
   ],
   "source": [
    "result = generate_with_single_input(**kwargs)\n",
    "print(result['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens used in the query is: 1900\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total tokens used in the query is: {total_tokens + result['usage']['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Route query (FAQ vs Product) and prepare final LLM kwargs\n",
    "\n",
    "Classifies the user query, calls the matching pipeline (`query_on_faq` or `query_on_products`), accumulates token usage, and sets the final `model` before returning `(kwargs, total_tokens)`. Includes fallbacks for undefined labels and product-pipeline errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def answer_query(query, model = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",simplified=False):\n",
    "    \"\"\"\n",
    "    Processes a user's query to determine its type (FAQ or Product) and executes the appropriate workflow.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The query string provided by the user.\n",
    "    - model (str): The model that will answer the question. Defaults to meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'\n",
    "    - simplified (bool): If True, uses a simplified version of the method. Defaults to False.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing keyword arguments for further processing.\n",
    "      If the query is neither FAQ nor Product-related, returns a default response dictionary instructing\n",
    "      the assistant to answer based on existing context.\n",
    "    \"\"\"\n",
    "    # Initialize the total tokens used to zero\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # Determine if the query is FAQ or Product and get the token count for this step\n",
    "    label, tokens = check_if_faq_or_product(query)\n",
    "    \n",
    "    # Sum the tokens\n",
    "    total_tokens += tokens\n",
    "    \n",
    "    # If the query is neither FAQ nor Product, return a default response\n",
    "    if label not in ['FAQ', 'Product']:\n",
    "        return {\n",
    "            \"role\": \"assistant\",\n",
    "            \"prompt\": (f\"User provided a question that does not fit FAQ or Product-related categories. \"\n",
    "                       f\"Answer it based on the context you already have. Query provided by the user: {query}\")\n",
    "        }\n",
    "    \n",
    "    # Process the query based on its label\n",
    "    if label == 'FAQ':\n",
    "        # Handle FAQ-related queries\n",
    "        kwargs = query_on_faq(query)\n",
    "    elif label == 'Product':\n",
    "        try:\n",
    "            # Handle Product-related queries, with error handling in place\n",
    "            kwargs, tokens = query_on_products(query)\n",
    "            # Add the tokens to the total tokens\n",
    "            total_tokens += tokens\n",
    "        except Exception:\n",
    "            # Return an error response if an exception occurs during querying\n",
    "            return {\n",
    "                \"role\": \"assistant\",\n",
    "                \"prompt\": (f\"User provided a question that broke the querying system. \"\n",
    "                           f\"Instruct them to rephrase it. Answer it based on the context you already have. \"\n",
    "                           f\"Query provided by the user: {query}\")\n",
    "            }, total_tokens\n",
    "    # Set the model to answer the final query - usually a better one         \n",
    "    kwargs['model'] = model\n",
    "    # Return the kwargs and total_tokens for further processing\n",
    "    return kwargs, total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ Final Output\n",
    "\n",
    "Runs the end-to-end pipeline for a user query:\n",
    "\n",
    "* Classifies the query (**FAQ** vs **Product**); if Product, also (**creative** vs **technical**).\n",
    "* Pulls top products via **semantic search** from Weaviate and composes a compact context.\n",
    "* Builds the final **prompt + decoding params** (`kwargs`) and reports `total_tokens`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "Client_Query = \"Give me three examples of blue t-shirts available on your catalogue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "kwargs, total_tokens = answer_query(Client_Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three examples of blue t-shirts available on the catalogue:\n",
      "\n",
      "1. Product ID: 21234. Product name: Basics Men Blue Printed T-shirt. Product Color: Blue. Product Season: Summer. Product Year: 2012.\n",
      "2. Product ID: 8317. Product name: Wildcraft Men Blue Printed T-shirt. Product Color: Blue. Product Season: Summer. Product Year: 2012.\n",
      "3. Product ID: 12211. Product name: Basics Men Blue Printed T-shirt. Product Color: Blue. Product Season: Fall. Product Year: 2011.\n"
     ]
    }
   ],
   "source": [
    "result = generate_with_single_input(**kwargs)\n",
    "print(result['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1951"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the total tokens for the call, we must sum the total_tokens to get the kwargs dictionary + total tokens from the LLM call\n",
    "total_tokens +  result['usage']['total_tokens']"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "rag-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
